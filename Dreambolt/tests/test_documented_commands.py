#!/usr/bin/env python3
"""
Test module for validating all documented commands in DreamBolt.
Auto-generated by DreamBolt Feature-QA Agent v1.

This module tests commands from:
- README.md
- testing.md 
- docs/**/*.md

Each test verifies that documented commands work as expected.
"""

import json
import subprocess
import time
from pathlib import Path
from typing import Dict, List
import pytest

# Project root and test data
PROJECT_ROOT = Path(__file__).parent.parent
COMMAND_INVENTORY_FILE = PROJECT_ROOT / "reports" / "command_inventory.json"
EXECUTION_RESULTS_FILE = PROJECT_ROOT / "reports" / "command_execution_results.json"

# Commands that are expected to fail (for error handling demos)
EXPECTED_FAILURES = {
    "python cli_working.py ingest nonexistent.csv",
    "python cli_working.py ingest test.txt", 
    "python cli_working.py ingest s3://test-bucket/WorldCupPlayers.csv",
    "python -m cli ingest s3://bucket/data.parquet --embed text-embedding-ada-002"
}

# Commands that require environment setup and should be skipped in tests
SKIP_COMMANDS = {
    "python -m venv dreambolt-env",
    "source dreambolt-env/bin/activate",
    "pip install -r requirements.txt",
    "echo \"test\" > test.txt",
    "rm test.txt"
}

def load_command_inventory() -> List[Dict]:
    """Load the command inventory from JSON file."""
    if COMMAND_INVENTORY_FILE.exists():
        with open(COMMAND_INVENTORY_FILE) as f:
            return json.load(f)
    return []

def load_execution_results() -> Dict[str, Dict]:
    """Load execution results, indexed by command."""
    results = {}
    if EXECUTION_RESULTS_FILE.exists():
        with open(EXECUTION_RESULTS_FILE) as f:
            data = json.load(f)
            for result in data:
                results[result["cmd"]] = result
    return results

# Load test data
COMMAND_INVENTORY = load_command_inventory()
EXECUTION_RESULTS = load_execution_results()

# Filter to executable commands only
EXECUTABLE_COMMANDS = [
    cmd for cmd in COMMAND_INVENTORY 
    if cmd["cmd"] not in SKIP_COMMANDS
]

@pytest.mark.e2e
class TestDocumentedCommands:
    """Test suite for all documented commands in DreamBolt."""
    
    @pytest.mark.parametrize("command_info", EXECUTABLE_COMMANDS)
    def test_documented_command_execution(self, command_info):
        """Test that documented commands execute as expected."""
        cmd = command_info["cmd"]
        origin_file = command_info["origin_file"]
        section = command_info["section"]
        
        # Skip commands that require special setup
        if cmd in SKIP_COMMANDS:
            pytest.skip(f"Skipping setup command: {cmd}")
            
        # Get expected results from execution log
        if cmd not in EXECUTION_RESULTS:
            pytest.fail(f"No execution results found for command: {cmd}")
            
        expected = EXECUTION_RESULTS[cmd]
        expected_exit_code = expected["exit_code"]
        
        # Execute the command
        start_time = time.time()
        try:
            result = subprocess.run(
                cmd.split(),
                cwd=PROJECT_ROOT,
                capture_output=True,
                text=True,
                timeout=60
            )
            execution_time = time.time() - start_time
            
        except subprocess.TimeoutExpired:
            pytest.fail(f"Command timed out: {cmd}")
        except Exception as e:
            pytest.fail(f"Command execution failed: {cmd}, Error: {str(e)}")
        
        # Verify exit code matches expectation
        if cmd in EXPECTED_FAILURES:
            # These commands should fail (error handling tests)
            assert result.returncode != 0, (
                f"Expected command to fail but it succeeded: {cmd}\n"
                f"From: {origin_file} ({section})\n"
                f"Stdout: {result.stdout}\n"
                f"Stderr: {result.stderr}"
            )
        else:
            # These commands should succeed
            assert result.returncode == expected_exit_code, (
                f"Command exit code mismatch: {cmd}\n"
                f"From: {origin_file} ({section})\n"
                f"Expected: {expected_exit_code}, Got: {result.returncode}\n"
                f"Stdout: {result.stdout}\n"
                f"Stderr: {result.stderr}"
            )
        
        # Verify consistent execution time (should be reasonable)
        assert execution_time < 30, (
            f"Command took too long: {cmd} ({execution_time:.1f}s)\n"
            f"Expected execution time from log: {expected.get('execution_time', 'unknown'):.1f}s"
        )
        
        # For successful commands, verify key output patterns
        if result.returncode == 0 and cmd not in EXPECTED_FAILURES:
            self._verify_command_output(cmd, result.stdout, result.stderr, origin_file, section)
    
    def _verify_command_output(self, cmd: str, stdout: str, stderr: str, origin_file: str, section: str):
        """Verify command output contains expected patterns."""
        
        # Common patterns that should appear in DreamBolt output
        if "cli_working.py" in cmd:
            if "status" in cmd:
                assert "DreamBolt Status Check" in stdout, (
                    f"Status command should show DreamBolt branding: {cmd}"
                )
                assert "âœ…" in stdout, (
                    f"Status command should show success indicators: {cmd}" 
                )
                
            elif "ingest" in cmd and "help" not in cmd:
                assert "ðŸš€ Starting DreamBolt ingestion" in stdout, (
                    f"Ingest command should show starting message: {cmd}"
                )
                assert "âœ… Ingestion completed successfully!" in stdout, (
                    f"Ingest command should show completion message: {cmd}"
                )
                
            elif "--help" in cmd:
                assert "usage:" in stdout, (
                    f"Help command should show usage: {cmd}"
                )
                assert "DreamBolt" in stdout, (
                    f"Help should mention DreamBolt: {cmd}"
                )

    @pytest.mark.e2e
    def test_command_inventory_completeness(self):
        """Test that command inventory covers all major functionality."""
        
        # Count commands by category
        categories = {}
        for cmd_info in COMMAND_INVENTORY:
            section = cmd_info["section"]
            categories[section] = categories.get(section, 0) + 1
            
        # Verify we have commands for major sections
        required_sections = [
            "Environment Setup",
            "CLI Functionality Testing", 
            "Data Ingestion Testing",
            "Error Handling Testing"
        ]
        
        for section in required_sections:
            assert section in categories, (
                f"Missing commands for required section: {section}"
            )
            assert categories[section] > 0, (
                f"No commands found for section: {section}"
            )
    
    @pytest.mark.e2e
    def test_execution_results_consistency(self):
        """Test that execution results are consistent and complete."""
        
        # Verify we have results for all executable commands
        executable_cmd_strings = {cmd["cmd"] for cmd in EXECUTABLE_COMMANDS}
        result_cmd_strings = set(EXECUTION_RESULTS.keys())
        
        # Commands in inventory should have results
        missing_results = executable_cmd_strings - result_cmd_strings
        assert not missing_results, (
            f"Missing execution results for commands: {missing_results}"
        )
        
        # Verify result structure
        for cmd, result in EXECUTION_RESULTS.items():
            required_fields = ["cmd", "exit_code", "stdout", "stderr", "execution_time", "status"]
            for field in required_fields:
                assert field in result, (
                    f"Missing field '{field}' in results for command: {cmd}"
                )
                
            # Verify status consistency  
            if result["exit_code"] == 0:
                assert result["status"] == "success", (
                    f"Status should be 'success' for exit code 0: {cmd}"
                )
            else:
                assert result["status"] == "failed", (
                    f"Status should be 'failed' for non-zero exit code: {cmd}"
                )

    @pytest.mark.e2e 
    def test_performance_benchmarks(self):
        """Test that commands meet reasonable performance benchmarks."""
        
        # Performance expectations (in seconds)
        performance_limits = {
            "help": 1.0,      # Help commands should be fast
            "status": 1.0,    # Status check should be fast  
            "ingest": 10.0,   # Ingestion can take longer
            "synthesis": 10.0, # Synthesis can take longer
            "embed": 15.0     # Embedding generation can take longer
        }
        
        slow_commands = []
        
        for cmd, result in EXECUTION_RESULTS.items():
            if result["status"] != "success":
                continue
                
            execution_time = result["execution_time"]
            
            # Categorize command type
            cmd_type = "other"
            if "--help" in cmd or "help" in cmd:
                cmd_type = "help"
            elif "status" in cmd:
                cmd_type = "status"
            elif "--embed" in cmd:
                cmd_type = "embed"
            elif "--synthesize" in cmd or "--synth" in cmd:
                cmd_type = "synthesis" 
            elif "ingest" in cmd:
                cmd_type = "ingest"
                
            # Check against limit
            limit = performance_limits.get(cmd_type, 5.0)
            if execution_time > limit:
                slow_commands.append(f"{cmd} ({execution_time:.1f}s > {limit}s)")
        
        assert not slow_commands, (
            "Commands exceeded performance limits:\n" + 
            "\n".join(slow_commands)
        )

    @pytest.mark.e2e
    def test_error_handling_completeness(self):
        """Test that error handling covers expected failure scenarios."""
        
        # Verify we test error conditions
        error_scenarios = []
        for cmd_info in EXECUTABLE_COMMANDS:
            cmd = cmd_info["cmd"]
            if cmd in EXPECTED_FAILURES:
                result = EXECUTION_RESULTS[cmd]
                assert result["status"] == "failed", (
                    f"Expected failure scenario should actually fail: {cmd}"
                )
                error_scenarios.append(cmd)
        
        # Should have at least these error scenarios
        expected_error_types = [
            "nonexistent",    # File not found
            ".txt",          # Unsupported format  
            "s3://",         # S3 disabled
        ]
        
        found_error_types = []
        for scenario in error_scenarios:
            for error_type in expected_error_types:
                if error_type in scenario:
                    found_error_types.append(error_type)
                    
        missing_error_types = set(expected_error_types) - set(found_error_types)
        assert not missing_error_types, (
            f"Missing error handling tests for: {missing_error_types}"
        )


# Standalone test functions for specific command patterns
@pytest.mark.e2e
def test_cli_help_system():
    """Test the CLI help system works comprehensively."""
    help_commands = [
        "python cli_working.py --help",
        "python cli_working.py ingest --help"
    ]
    
    for cmd in help_commands:
        if cmd in EXECUTION_RESULTS:
            result_data = EXECUTION_RESULTS[cmd]
            assert result_data["status"] == "success", f"Help command failed: {cmd}"
            assert "usage:" in result_data["stdout"], f"Help should show usage: {cmd}"


@pytest.mark.e2e
def test_basic_ingestion_workflow():
    """Test the complete basic ingestion workflow."""
    workflow_commands = [
        "python cli_working.py status",
        "python cli_working.py ingest WorldCupPlayers.csv --no-synth"
    ]
    
    for cmd in workflow_commands:
        if cmd in EXECUTION_RESULTS:
            result_data = EXECUTION_RESULTS[cmd]
            assert result_data["status"] == "success", f"Workflow command failed: {cmd}"
            
    # Verify output file was created
    output_file = PROJECT_ROOT / "WorldCupPlayers.dreambolt.parquet"
    assert output_file.exists(), "Ingestion should create output file"


@pytest.mark.e2e 
def test_advanced_features():
    """Test advanced features like synthesis and embeddings."""
    advanced_commands = [
        cmd for cmd in EXECUTION_RESULTS.keys()
        if ("--synthesize" in cmd or "--embed" in cmd) and 
           cmd not in EXPECTED_FAILURES
    ]
    
    assert len(advanced_commands) > 0, "Should have advanced feature commands"
    
    for cmd in advanced_commands:
        result_data = EXECUTION_RESULTS[cmd]
        assert result_data["status"] == "success", f"Advanced feature failed: {cmd}"


if __name__ == "__main__":
    # Run tests when executed directly
    pytest.main([__file__, "-v", "-m", "e2e"]) 